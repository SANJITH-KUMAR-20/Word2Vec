{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import regex as re\n",
        "import random"
      ],
      "metadata": {
        "id": "b89xoyU2leOy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec:\n",
        "\n",
        "  def __init__(self,input_file_path,stop_words = None):\n",
        "    self.input_file_path = input_file_path\n",
        "    self.word_count = None\n",
        "    self.count = 0\n",
        "    #self.vocab_size = None\n",
        "    self.stop_words = stop_words\n",
        "    self.word_to_index = {}\n",
        "    self.index_to_word = {}\n",
        "    self.vocab = []\n",
        "\n",
        "    #self._target_words = []\n",
        "    #self._context_vectors = []\n",
        "    #self._target_to_context_data = {}\n",
        "\n",
        "    self.data = self._read_file(self.input_file_path)\n",
        "    self._Prepare_data_utils(self.data)\n",
        "    self.vocab = self.vocab[:1000]\n",
        "    self.word_count = len(self.vocab)\n",
        "\n",
        "  def process(self,window_size):\n",
        "    #data = self._read_file(self.input_file_path)\n",
        "    #self._Prepare_data_utils(data)\n",
        "    return self._generate_training_data(window_size)\n",
        "  def _read_file(self,remove_stop_words = False):\n",
        "    file_contents = []\n",
        "    if os.path.exists(self.input_file_path):\n",
        "\n",
        "      with open(self.input_file_path) as f:\n",
        "          file_contents = f.read()\n",
        "      data = []\n",
        "      for sent in file_contents.split('.'):\n",
        "          sent = re.findall(\"[A-Za-z]+\", sent)\n",
        "          new_sent = ''\n",
        "          for words in sent:\n",
        "\n",
        "              if self.stop_words is not None:\n",
        "                  if len(words) > 1 and words not in self.stop_words:\n",
        "                      new_sent = new_sent + ' ' + words\n",
        "                  continue\n",
        "              if len(words) > 1 :\n",
        "                    new_sent = new_sent + ' ' + words\n",
        "          data.append(new_sent)\n",
        "      return data\n",
        "    else:\n",
        "      raise Exception(\"File Path Does Not Exist\")\n",
        "\n",
        "  def _Prepare_data_utils(self,data):\n",
        "    for sent in data:\n",
        "        for word in sent.split():\n",
        "            word = word.lower()\n",
        "            self.vocab.append(word)\n",
        "            if word not in self.word_to_index:\n",
        "                self.word_to_index[word] = self.count\n",
        "                self.index_to_word[self.count] = word\n",
        "                self.count  += 1\n",
        "    self.word_count = len(self.vocab)\n",
        "  def _one_hot_encode(self,target_word,context_words):\n",
        "    target_vector = np.zeros(len(self.vocab))\n",
        "    context_vector = np.zeros(len(self.vocab))\n",
        "    target_index = self.word_to_index.get(target_word)\n",
        "    for word in context_words:\n",
        "      context_index = self.word_to_index.get(word)\n",
        "      context_vector[context_index] = 1\n",
        "    target_vector[target_index] = 1\n",
        "    return target_vector,context_vector\n",
        "  def _generate_training_data(self,window_size,gen_negative_data = True):\n",
        "    target_vectors, context_vectors, labels = [],[],[]\n",
        "    if gen_negative_data:\n",
        "      for index,word in enumerate(self.vocab):\n",
        "        target = word\n",
        "        context_words = random.sample(self.vocab,window_size*2)\n",
        "        target_vector,context_vector = self._one_hot_encode(target,context_words)\n",
        "        labels.append(0)\n",
        "        target_vectors.append(target_vector)\n",
        "        context_vectors.append(context_vector)\n",
        "\n",
        "\n",
        "    for index,word in enumerate(self.vocab):\n",
        "      target = word\n",
        "      context_words = []\n",
        "      if index == 0:\n",
        "        context_words = [self.vocab[idx] for idx in range(index+1,index+1+window_size)]\n",
        "      elif index == self.word_count - 1:\n",
        "        context_words = [self.vocab[idx] for idx in range(index-1,index-1-window_size,-1)]\n",
        "      else:\n",
        "        #right side\n",
        "        for idx in range(index+1,index+1+window_size):\n",
        "          if idx < len(self.vocab)-1:\n",
        "            #print(index)\n",
        "            context_words.append(self.vocab[idx])\n",
        "            continue\n",
        "          break\n",
        "\n",
        "        #left side\n",
        "        for idx in range(index-1,index-1-window_size,-1):\n",
        "          if idx > 0:\n",
        "            context_words.append(self.vocab[idx])\n",
        "            continue\n",
        "          break\n",
        "      target_vector,context_vector = self._one_hot_encode(target,context_words)\n",
        "      labels.append(1)\n",
        "      target_vectors.append(target_vector)\n",
        "      context_vectors.append(context_vector)\n",
        "\n",
        "    return np.array(target_vectors), np.array(context_vectors), np.array(labels)\n"
      ],
      "metadata": {
        "id": "TDHgYUK_mNi3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "vectorizer = Word2Vec(path_to_file)\n",
        "Autotune = tf.data.AUTOTUNE\n",
        "target_vectors,context_vectors,labels = vectorizer.process(2)\n",
        "data = tf.data.Dataset.from_tensor_slices(((target_vectors,context_vectors),labels))\n",
        "data = data.cache().shuffle(5000).batch(1000).prefetch(Autotune)"
      ],
      "metadata": {
        "id": "g4U8ujTQmh0Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,vocab_size,emb_dim):\n",
        "    super(Word2VecModel,self).__init__()\n",
        "    self.target_embedding = tf.keras.layers.Embedding(vocab_size,emb_dim,name = \"embedding_1\")\n",
        "\n",
        "    self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                       emb_dim,\n",
        "                                       name = \"embedding_2\")\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense = tf.keras.layers.Dense(1,activation = \"sigmoid\")\n",
        "\n",
        "  def call(self,x):\n",
        "    target,context = x\n",
        "    word_em1 = self.target_embedding(target)\n",
        "    word_em2 = self.context_embedding(context)\n",
        "    dots = tf.math.add(word_em1,word_em2)\n",
        "    dots = self.flatten(dots)\n",
        "    dots = self.dense(dots)\n",
        "    return dots"
      ],
      "metadata": {
        "id": "be5G2wW2ml1d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = Word2VecModel(1000,120)\n",
        "my_model.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits = False),\n",
        "                 metrics=['accuracy'])\n",
        "my_model.fit(data,epochs = 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_yDoKzumu5A",
        "outputId": "a15f2697-79b6-4ecd-bb24-b07fce7e3a6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 6s 79ms/step - loss: 1.3853 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.9783 - accuracy: 0.5030\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 1.2914 - accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.7576 - accuracy: 0.4920\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.9657 - accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.8535 - accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.7012 - accuracy: 0.5120\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.7933 - accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.7884 - accuracy: 0.5000\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.7096 - accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.7018 - accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.7290 - accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.7277 - accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.7047 - accuracy: 0.5000\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.6942 - accuracy: 0.4890\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.6976 - accuracy: 0.5000\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.7051 - accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.7034 - accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.6969 - accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.6931 - accuracy: 0.5405\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c12f8754550>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = my_model.get_layer('embedding_1').get_weights()[0]"
      ],
      "metadata": {
        "id": "tRCGdEROm32R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(f\"{vectorizer.vocab[i]} --> {weights[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9IzlNj6nEiD",
        "outputId": "b223913d-b3ed-4498-c20f-e22568dbc802"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first --> [-0.0170556  -0.01243787  0.00800631 -0.0252011   0.00590843 -0.00553115\n",
            " -0.01863282 -0.01254865  0.0119739  -0.04019622  0.00842682  0.01008526\n",
            " -0.01437738 -0.01867424  0.03809348  0.01732139  0.02572223 -0.0023969\n",
            " -0.02887852  0.01013532  0.03384162  0.02036899 -0.00526825 -0.01088508\n",
            " -0.00495841 -0.0413136  -0.01216512  0.01178444  0.01038048  0.03569179\n",
            "  0.02515993  0.02015463 -0.01342836 -0.02670871  0.00222888 -0.01042104\n",
            "  0.0108773   0.00628067  0.01688491  0.02201271 -0.02625884  0.03790294\n",
            " -0.00548539 -0.01699776 -0.00224262 -0.00725609 -0.02122648 -0.00460905\n",
            " -0.02242819  0.00616083 -0.01072491  0.01072041 -0.02837901 -0.01030625\n",
            " -0.04010846  0.01760278 -0.0307328   0.00995832  0.0252569   0.01456528\n",
            "  0.0024913  -0.03257979  0.03858005  0.0247371   0.02406243  0.00557162\n",
            "  0.04629162  0.00309694  0.03102067  0.0171019   0.00737533 -0.01972257\n",
            " -0.00252412 -0.03328662  0.01405191  0.00577006 -0.02121025  0.00300525\n",
            " -0.01443823 -0.02820752  0.01292439 -0.02850588  0.01928901 -0.01079965\n",
            " -0.00016949  0.04115171 -0.00987439  0.02608586  0.00043849  0.00631119\n",
            "  0.0113343  -0.01889848 -0.00147094 -0.025294    0.01246531 -0.00676282\n",
            "  0.04664848  0.04116414 -0.01303618  0.03443395 -0.00744014 -0.01539232\n",
            " -0.00937491  0.02133057  0.03972686  0.00905992 -0.0016616  -0.00631307\n",
            "  0.02194993  0.01164964 -0.02090038 -0.03019253 -0.01221583  0.03414745\n",
            " -0.00910805  0.01025902  0.03371742 -0.03365109  0.00671455  0.01657899]\n",
            "citizen --> [-0.00756498 -0.05388671 -0.05811144 -0.03816374 -0.01120272  0.04446082\n",
            " -0.02158339 -0.02689309 -0.00940972 -0.02559194  0.05442482 -0.02095744\n",
            " -0.02102326  0.0335359  -0.02556356  0.04503163  0.02294463  0.01380577\n",
            "  0.03599953  0.00229724 -0.02745117 -0.02328761 -0.04706387  0.02242227\n",
            "  0.03860388 -0.01268309 -0.00187962  0.00259521 -0.03490071 -0.0454971\n",
            "  0.00961674 -0.0409921   0.02171963 -0.0142826  -0.01680979  0.01303079\n",
            " -0.04437687  0.02732012  0.02538704 -0.01457468  0.04056426 -0.03888865\n",
            "  0.03833408  0.06180957  0.03405073 -0.01018998 -0.02586321 -0.04463695\n",
            "  0.01059235 -0.02470069  0.02080628  0.00742355 -0.01862552  0.03179245\n",
            "  0.00735     0.03071507 -0.02436938 -0.01514724 -0.06096228  0.02606785\n",
            "  0.0252599   0.06611159 -0.02173462 -0.00032456  0.06425261 -0.04210385\n",
            " -0.02683311  0.03168845 -0.00844592 -0.05671062  0.02885846  0.01987399\n",
            " -0.03359773 -0.02642353  0.046283   -0.03132077  0.01076033  0.0413134\n",
            " -0.02856958  0.01558236 -0.00101634 -0.00370046  0.00768922  0.03652887\n",
            "  0.02856537 -0.0475818  -0.01747405  0.04566435 -0.01940159 -0.00940869\n",
            "  0.03888628 -0.02674221  0.03619362 -0.04848307 -0.02156147 -0.04635704\n",
            " -0.0015954  -0.01505714  0.00503022 -0.05943764 -0.02297135  0.05507094\n",
            " -0.05090911  0.00542936 -0.0040159   0.00667356  0.01165004  0.01422186\n",
            "  0.03533781 -0.04231414 -0.04392238  0.04244663  0.03763313  0.01752406\n",
            "  0.03540716  0.03168273 -0.0354936   0.01465892  0.01179074 -0.02328758]\n",
            "before --> [ 0.04039825  0.04881454  0.02211631 -0.02978342  0.03499082 -0.02269419\n",
            "  0.03922962 -0.01910322 -0.04803108 -0.02480946  0.03710318  0.0161784\n",
            "  0.03346336  0.01243575 -0.00712209 -0.01075165  0.00924783 -0.03003514\n",
            "  0.04242272  0.04371807 -0.03543929  0.04687735 -0.02507316  0.03486105\n",
            "  0.04325003  0.00647596 -0.04806293  0.01082123  0.04656451 -0.04859792\n",
            " -0.04963921 -0.03200001 -0.03734686 -0.03929576 -0.04638752  0.03107734\n",
            "  0.04472012  0.02838634  0.01321645  0.01489469 -0.01865914  0.02082492\n",
            " -0.00491957  0.04238153  0.04643096 -0.01730603  0.03798563 -0.03609945\n",
            "  0.01473964 -0.03770013  0.04809353 -0.01531468 -0.00412357 -0.02214754\n",
            " -0.00077915 -0.02963952  0.00626279  0.04785186  0.01900143  0.0335888\n",
            "  0.02537536  0.01205448 -0.03977759  0.02124793 -0.04442116  0.04200807\n",
            " -0.00910439 -0.01220452  0.01568926  0.02039459  0.04661519 -0.04325665\n",
            " -0.00185172 -0.03901467  0.03815254 -0.00950667 -0.03002923 -0.00493102\n",
            " -0.03539145 -0.03853097  0.03474141  0.01572073  0.0125342  -0.01892119\n",
            " -0.03959575 -0.04188236  0.03807956  0.02726376 -0.0470853  -0.03779889\n",
            "  0.03790839  0.04202129 -0.01572769  0.0101161  -0.00986874 -0.03507258\n",
            " -0.04641317 -0.03563441 -0.02683566  0.00224879  0.04908485  0.0006169\n",
            "  0.00743048 -0.02895262  0.01190815 -0.01527779 -0.00475823  0.03386931\n",
            "  0.0475362   0.02209545 -0.02142507 -0.01970758 -0.0395506   0.038905\n",
            "  0.01491399  0.04210189 -0.03648206 -0.03333423 -0.0310807   0.01462764]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fF-LZWuBndQp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}