{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlyBoaadyMs9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import regex as re\n",
        "\n",
        "class Word2Vec:\n",
        "\n",
        "  def __init__(self,input_file_path,stop_words = None):\n",
        "    self.input_file_path = input_file_path\n",
        "    self.word_count = None\n",
        "    self.count = 0\n",
        "    #self.vocab_size = None\n",
        "    self.stop_words = stop_words\n",
        "    self.word_to_index = {}\n",
        "    self.index_to_word = {}\n",
        "    self.vocab = []\n",
        "\n",
        "    #self._target_words = []\n",
        "    #self._context_vectors = []\n",
        "    #self._target_to_context_data = {}\n",
        "\n",
        "    self.data = self._read_file(self.input_file_path)\n",
        "    self._Prepare_data_utils(self.data)\n",
        "    self.vocab = self.vocab[:500]\n",
        "    self.word_count = len(self.vocab)\n",
        "\n",
        "  def process(self,window_size):\n",
        "    #data = self._read_file(self.input_file_path)\n",
        "    #self._Prepare_data_utils(data)\n",
        "    return self._generate_training_data(window_size)\n",
        "    #return self._Augmented_Generated_data(target_vec,context_vec,labels,window_size)\n",
        "\n",
        "  def _read_file(self,remove_stop_words = False):\n",
        "    file_contents = []\n",
        "    if os.path.exists(self.input_file_path):\n",
        "\n",
        "      with open(self.input_file_path) as f:\n",
        "          file_contents = f.read()\n",
        "      data = []\n",
        "      for sent in file_contents.split('.'):\n",
        "          sent = re.findall(\"[A-Za-z]+\", sent)\n",
        "          new_sent = ''\n",
        "          for words in sent:\n",
        "\n",
        "              if self.stop_words is not None:\n",
        "                  if len(words) > 1 and words not in self.stop_words:\n",
        "                      new_sent = new_sent + ' ' + words\n",
        "                  continue\n",
        "              if len(words) > 1 :\n",
        "                    new_sent = new_sent + ' ' + words\n",
        "          data.append(new_sent)\n",
        "      return data\n",
        "    else:\n",
        "      raise Exception(\"File Path Does Not Exist\")\n",
        "\n",
        "  def _Prepare_data_utils(self,data):\n",
        "    for sent in data:\n",
        "        for word in sent.split():\n",
        "            word = word.lower()\n",
        "            self.vocab.append(word)\n",
        "            if word not in self.word_to_index:\n",
        "                self.word_to_index[word] = self.count\n",
        "                self.index_to_word[self.count] = word\n",
        "                self.count  += 1\n",
        "    self.word_count = len(self.vocab)\n",
        "\n",
        "  def _one_hot_encode(self,target_word,context_words):\n",
        "    target_vector = np.zeros(len(self.vocab))\n",
        "    context_vector = np.zeros(len(self.vocab))\n",
        "    target_index = self.word_to_index.get(target_word)\n",
        "    for word in context_words:\n",
        "      context_index = self.word_to_index.get(word)\n",
        "      context_vector[context_index] = 1\n",
        "    target_vector[target_index] = 1\n",
        "    return target_vector,context_vector\n",
        "\n",
        "  def _generate_training_data(self,window_size,gen_negative_data = True):\n",
        "    target_vectors, context_vectors, labels = [],[],[]\n",
        "    if gen_negative_data:\n",
        "      for index,word in enumerate(self.vocab):\n",
        "        target = word\n",
        "        context_words = random.sample(self.vocab,window_size*2)\n",
        "        target_vector,context_vector = self._one_hot_encode(target,context_words)\n",
        "        labels.append([0])\n",
        "        target_vectors.append(target_vector)\n",
        "        context_vectors.append(context_vector)\n",
        "\n",
        "\n",
        "    for index,word in enumerate(self.vocab):\n",
        "      target = word\n",
        "      context_words = []\n",
        "      if index == 0:\n",
        "        context_words = [self.vocab[idx] for idx in range(index+1,index+1+window_size)]\n",
        "      elif index == self.word_count - 1:\n",
        "        context_words = [self.vocab[idx] for idx in range(index-1,index-1-window_size,-1)]\n",
        "      else:\n",
        "        #right side\n",
        "        for idx in range(index+1,index+1+window_size):\n",
        "          if idx < len(self.vocab)-1:\n",
        "            #print(index)\n",
        "            context_words.append(self.vocab[idx])\n",
        "            continue\n",
        "          break\n",
        "\n",
        "        #left side\n",
        "        for idx in range(index-1,index-1-window_size,-1):\n",
        "          if idx > 0:\n",
        "            context_words.append(self.vocab[idx])\n",
        "            continue\n",
        "          break\n",
        "      target_vector,context_vector = self._one_hot_encode(target,context_words)\n",
        "      labels.append([1])\n",
        "      target_vectors.append(target_vector)\n",
        "      context_vectors.append(context_vector)\n",
        "\n",
        "    return np.array(target_vectors), np.array(context_vectors), np.array(labels)\n",
        "\n",
        "  '''def _Get_Target_Vectors(self,target_words,target,window_size):\n",
        "    target_vec = [target]\n",
        "    labels = [1]\n",
        "    for idx,word in enumerate(target_words):\n",
        "      target_vector = np.zeros(len(self.vocab))\n",
        "      target_vector[self.word_to_index[word]] = 1\n",
        "      target_vec.append(target_vector)\n",
        "      if np.all(target_vector == np.array(target)):\n",
        "        labels.append(1)\n",
        "      else:\n",
        "        labels.append(0)\n",
        "    return target_vec,labels\n",
        "\n",
        "\n",
        "  def _Augmented_Generated_data(self,target_vectors,context_vectors,labels,window_size):\n",
        "    new_context = []\n",
        "    new_targets = []\n",
        "    new_labels = []\n",
        "    for target,contexts in zip(target_vectors,context_vectors):\n",
        "      for idx,context in enumerate(contexts):\n",
        "        cont_vec = np.zeros(len(self.vocab))\n",
        "        if context == 1:\n",
        "            cont_vec[idx] = 1\n",
        "            new_context.append(cont_vec)\n",
        "            target_words = random.sample(self.vocab,window_size*2)\n",
        "            target_vector,label = self._Get_Target_Vectors(target_words,target,window_size)\n",
        "            new_targets.append(target_vector)\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return np.array(new_context),np.array(new_targets),np.array(new_labels)'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "vectorizer = Word2Vec(path_to_file)\n",
        "Autotune = tf.data.AUTOTUNE\n",
        "target_vectors,context_vectors,labels = vectorizer.process(2)\n",
        "data = tf.data.Dataset.from_tensor_slices((context_vectors,(target_vectors,labels)))\n",
        "data = data.cache().shuffle(5000).batch(500).prefetch(Autotune)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BTQDz1EE5h9",
        "outputId": "ff94cc16-887c-427f-a1b9-52ca0d5f1277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Word2VecCBOW_Model(Vocab_size,Hidden_dim):\n",
        "  Inp_Layer = tf.keras.layers.Input((Vocab_size,),name = \"input_layer\")\n",
        "  Embedding_Layer = tf.keras.layers.Embedding(Vocab_size,Hidden_dim,name = \"Embedding_Layer_1\")(Inp_Layer)\n",
        "  print(Embedding_Layer.shape)\n",
        "  Comm_Hidden_Layer = tf.keras.layers.Dense(128,activation = \"relu\",name = \"Common_hidden\")(Embedding_Layer)\n",
        "  print(Comm_Hidden_Layer.shape)\n",
        "\n",
        "  #For_Target\n",
        "  Target_Hidden = tf.keras.layers.Dense(64,activation=\"relu\",name = \"Target_hidden\")(Comm_Hidden_Layer)\n",
        "  Reg = tf.keras.layers.Dropout(0.1,name = \"Regularization_1\")(Target_Hidden)\n",
        "  Target = tf.keras.layers.Dense(Vocab_size,name = \"Target_Out\")(Reg)\n",
        "\n",
        "  #For_Neg_or_Pos_Labels\n",
        "  Label_Hidden = tf.keras.layers.Dense(64,activation=\"relu\",name = \"Label_hidden\")(Comm_Hidden_Layer)\n",
        "  Reg2 = tf.keras.layers.Dropout(0.1,name = \"Regularization_2\")(Label_Hidden)\n",
        "  Label = tf.keras.layers.Dense(1,name = \"Label_Out\")(Reg2)\n",
        "  print(Label.shape)\n",
        "\n",
        "  CBOWWord2Vec = tf.keras.models.Model(inputs = Inp_Layer,outputs = [Target,Label])\n",
        "\n",
        "  return CBOWWord2Vec"
      ],
      "metadata": {
        "id": "EqCtyQIoj5AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecModel(tf.keras.models.Model):\n",
        "\n",
        "  def __init__(self,my_model,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.model = my_model\n",
        "\n",
        "  def compile(self,optimizer,Target_loss,Label_loss,**kwargs):\n",
        "    super().compile(**kwargs)\n",
        "    self.optimizer = optimizer\n",
        "    self.Target_loss = Target_loss\n",
        "    self.Label_loss = Label_loss\n",
        "\n",
        "  def train_step(self,batch,**kwargs):\n",
        "    x,y = batch\n",
        "    print(x.shape)\n",
        "    print(y[0].shape,y[1].shape)\n",
        "    with tf.GradientTape() as tape:\n",
        "      Target,Label = self.model(x,training = True)\n",
        "      batch_targetloss = self.Target_loss(tf.cast(y[0],tf.float32),Target[0])\n",
        "      batch_labelloss = self.Label_loss(tf.cast(y[1],tf.float32),Label[0])\n",
        "      total_loss = batch_targetloss + batch_labelloss\n",
        "\n",
        "      gradients = tape.gradient(total_loss,self.model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients,self.model.trainable_variables))\n",
        "\n",
        "    return {\"total_loss\" : total_loss,\"Target_loss\":batch_targetloss,\"Label_loss\":batch_labelloss}\n",
        "\n",
        "  def test_step(self,batch,**kwargs):\n",
        "    x,y = batch\n",
        "\n",
        "    Target,Label = self.model(x,training = True)\n",
        "\n",
        "    batch_targetloss = self.Target_loss(y[0],Target)\n",
        "    batch_labelloss = self.Label_loss(y[1],Label[0])\n",
        "    total_loss = batch_targetloss + batch_labelloss\n",
        "    return {\"total_loss\" : total_loss,\"Target_loss\":batch_targetloss,\"Label_loss\":batch_labelloss}\n",
        "\n",
        "  def call(self,inp,**kwargs):\n",
        "    return self.model(inp,**kwargs)"
      ],
      "metadata": {
        "id": "TdVt8zPCndCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2VecCBOW_Model(500,120)\n",
        "model = Word2VecModel(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhnt-kSntFue",
        "outputId": "395b850a-5e36-4444-fff0-41d66937aa90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 500, 120)\n",
            "(None, 500, 128)\n",
            "(None, 500, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def target_loss(y_true,y_pred):\n",
        "  return tf.nn.softmax_cross_entropy_with_logits(y_true,y_pred)\n",
        "def label_loss(y_true,y_pred):\n",
        "  return tf.nn.sigmoid_cross_entropy_with_logits(y_true,y_pred)"
      ],
      "metadata": {
        "id": "fWq92Lkdw9IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),Target_loss = target_loss,Label_loss=label_loss,metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "eqNRnEmTtlK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(data,epochs = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhAWefaHuFDS",
        "outputId": "6ffb3f5b-db34-407d-b7f8-f5b0dd9f2a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "(None, 500)\n",
            "(None, 500) (None, 1)\n",
            "(None, 500)\n",
            "(None, 500) (None, 1)\n",
            "2/2 [==============================] - 7s 89ms/step - total_loss: 6.9064 - Target_loss: 6.2132 - Label_loss: 0.6932\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 72ms/step - total_loss: 6.9013 - Target_loss: 6.2080 - Label_loss: 0.6934\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 68ms/step - total_loss: 6.8953 - Target_loss: 6.2023 - Label_loss: 0.6930\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 6.8885 - Target_loss: 6.1952 - Label_loss: 0.6933\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 6.8790 - Target_loss: 6.1860 - Label_loss: 0.6930\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 6.8683 - Target_loss: 6.1745 - Label_loss: 0.6938\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 6.8542 - Target_loss: 6.1611 - Label_loss: 0.6931\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 6.8364 - Target_loss: 6.1432 - Label_loss: 0.6932\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 6.8144 - Target_loss: 6.1210 - Label_loss: 0.6934\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 68ms/step - total_loss: 6.7853 - Target_loss: 6.0922 - Label_loss: 0.6930\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 6.7517 - Target_loss: 6.0585 - Label_loss: 0.6932\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 6.7070 - Target_loss: 6.0146 - Label_loss: 0.6923\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 68ms/step - total_loss: 6.6566 - Target_loss: 5.9621 - Label_loss: 0.6946\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 6.5934 - Target_loss: 5.8994 - Label_loss: 0.6940\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 6.5150 - Target_loss: 5.8220 - Label_loss: 0.6930\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 6.4356 - Target_loss: 5.7433 - Label_loss: 0.6924\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 6.3468 - Target_loss: 5.6550 - Label_loss: 0.6918\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 6.2314 - Target_loss: 5.5375 - Label_loss: 0.6939\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 6.1177 - Target_loss: 5.4229 - Label_loss: 0.6948\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 6.0278 - Target_loss: 5.3341 - Label_loss: 0.6937\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.9346 - Target_loss: 5.2399 - Label_loss: 0.6947\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.8789 - Target_loss: 5.1835 - Label_loss: 0.6954\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.8617 - Target_loss: 5.1684 - Label_loss: 0.6933\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.8470 - Target_loss: 5.1511 - Label_loss: 0.6959\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.8528 - Target_loss: 5.1593 - Label_loss: 0.6935\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.8333 - Target_loss: 5.1398 - Label_loss: 0.6935\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.8399 - Target_loss: 5.1440 - Label_loss: 0.6959\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.8018 - Target_loss: 5.1061 - Label_loss: 0.6957\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.7652 - Target_loss: 5.0682 - Label_loss: 0.6969\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 5.7682 - Target_loss: 5.0747 - Label_loss: 0.6935\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.7765 - Target_loss: 5.0814 - Label_loss: 0.6951\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7353 - Target_loss: 5.0423 - Label_loss: 0.6930\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7602 - Target_loss: 5.0660 - Label_loss: 0.6942\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7540 - Target_loss: 5.0589 - Label_loss: 0.6951\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7668 - Target_loss: 5.0746 - Label_loss: 0.6921\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 5.7498 - Target_loss: 5.0546 - Label_loss: 0.6952\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 97ms/step - total_loss: 5.7523 - Target_loss: 5.0585 - Label_loss: 0.6939\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 103ms/step - total_loss: 5.7191 - Target_loss: 5.0235 - Label_loss: 0.6956\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 90ms/step - total_loss: 5.7512 - Target_loss: 5.0579 - Label_loss: 0.6933\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 86ms/step - total_loss: 5.7497 - Target_loss: 5.0559 - Label_loss: 0.6938\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 5.7567 - Target_loss: 5.0600 - Label_loss: 0.6966\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 77ms/step - total_loss: 5.7424 - Target_loss: 5.0460 - Label_loss: 0.6964\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 85ms/step - total_loss: 5.7696 - Target_loss: 5.0759 - Label_loss: 0.6937\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 90ms/step - total_loss: 5.7411 - Target_loss: 5.0468 - Label_loss: 0.6944\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 86ms/step - total_loss: 5.7519 - Target_loss: 5.0575 - Label_loss: 0.6943\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 106ms/step - total_loss: 5.7400 - Target_loss: 5.0472 - Label_loss: 0.6929\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 88ms/step - total_loss: 5.7434 - Target_loss: 5.0498 - Label_loss: 0.6936\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7369 - Target_loss: 5.0437 - Label_loss: 0.6932\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7402 - Target_loss: 5.0473 - Label_loss: 0.6929\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 70ms/step - total_loss: 5.7329 - Target_loss: 5.0396 - Label_loss: 0.6933\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 69ms/step - total_loss: 5.7483 - Target_loss: 5.0557 - Label_loss: 0.6926\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 78ms/step - total_loss: 5.7399 - Target_loss: 5.0464 - Label_loss: 0.6935\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 5.7413 - Target_loss: 5.0488 - Label_loss: 0.6925\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 68ms/step - total_loss: 5.7442 - Target_loss: 5.0504 - Label_loss: 0.6937\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 71ms/step - total_loss: 5.7304 - Target_loss: 5.0358 - Label_loss: 0.6946\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 83ms/step - total_loss: 5.7183 - Target_loss: 5.0255 - Label_loss: 0.6928\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7507 - Target_loss: 5.0566 - Label_loss: 0.6941\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 5.7253 - Target_loss: 5.0316 - Label_loss: 0.6936\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7457 - Target_loss: 5.0526 - Label_loss: 0.6930\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 69ms/step - total_loss: 5.7212 - Target_loss: 5.0288 - Label_loss: 0.6924\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 5.7330 - Target_loss: 5.0400 - Label_loss: 0.6930\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7372 - Target_loss: 5.0427 - Label_loss: 0.6945\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7167 - Target_loss: 5.0247 - Label_loss: 0.6920\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7457 - Target_loss: 5.0532 - Label_loss: 0.6925\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 69ms/step - total_loss: 5.7502 - Target_loss: 5.0551 - Label_loss: 0.6951\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 5.7577 - Target_loss: 5.0645 - Label_loss: 0.6932\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7333 - Target_loss: 5.0407 - Label_loss: 0.6926\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7320 - Target_loss: 5.0373 - Label_loss: 0.6947\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 5.7423 - Target_loss: 5.0478 - Label_loss: 0.6945\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.7301 - Target_loss: 5.0375 - Label_loss: 0.6926\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 5.7499 - Target_loss: 5.0565 - Label_loss: 0.6933\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 5.7307 - Target_loss: 5.0372 - Label_loss: 0.6935\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 5.7665 - Target_loss: 5.0734 - Label_loss: 0.6932\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7366 - Target_loss: 5.0433 - Label_loss: 0.6933\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 5.7429 - Target_loss: 5.0494 - Label_loss: 0.6935\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7368 - Target_loss: 5.0434 - Label_loss: 0.6935\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 5.7332 - Target_loss: 5.0402 - Label_loss: 0.6930\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.7281 - Target_loss: 5.0339 - Label_loss: 0.6943\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 5.7414 - Target_loss: 5.0479 - Label_loss: 0.6935\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 5.7369 - Target_loss: 5.0436 - Label_loss: 0.6933\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7396 - Target_loss: 5.0462 - Label_loss: 0.6934\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 5.7136 - Target_loss: 5.0200 - Label_loss: 0.6936\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 64ms/step - total_loss: 5.7465 - Target_loss: 5.0530 - Label_loss: 0.6935\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 68ms/step - total_loss: 5.7529 - Target_loss: 5.0604 - Label_loss: 0.6925\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 5.7163 - Target_loss: 5.0233 - Label_loss: 0.6931\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7615 - Target_loss: 5.0688 - Label_loss: 0.6927\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 63ms/step - total_loss: 5.7283 - Target_loss: 5.0342 - Label_loss: 0.6941\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 5.7257 - Target_loss: 5.0321 - Label_loss: 0.6936\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7293 - Target_loss: 5.0356 - Label_loss: 0.6937\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7418 - Target_loss: 5.0484 - Label_loss: 0.6934\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 67ms/step - total_loss: 5.7387 - Target_loss: 5.0448 - Label_loss: 0.6938\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 68ms/step - total_loss: 5.7591 - Target_loss: 5.0656 - Label_loss: 0.6935\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 65ms/step - total_loss: 5.7294 - Target_loss: 5.0364 - Label_loss: 0.6930\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 70ms/step - total_loss: 5.7537 - Target_loss: 5.0600 - Label_loss: 0.6937\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 69ms/step - total_loss: 5.7147 - Target_loss: 5.0218 - Label_loss: 0.6929\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 70ms/step - total_loss: 5.7565 - Target_loss: 5.0632 - Label_loss: 0.6934\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 69ms/step - total_loss: 5.7407 - Target_loss: 5.0480 - Label_loss: 0.6927\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 72ms/step - total_loss: 5.7419 - Target_loss: 5.0481 - Label_loss: 0.6938\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 66ms/step - total_loss: 5.7266 - Target_loss: 5.0333 - Label_loss: 0.6934\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 69ms/step - total_loss: 5.7448 - Target_loss: 5.0518 - Label_loss: 0.6930\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a503b4d3fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.model.get_layer(\"Embedding_Layer_1\").get_weights()"
      ],
      "metadata": {
        "id": "ap4yT8q5ym7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoI4fcYoDhjf",
        "outputId": "c80992c8-736f-4c53-a4ec-a2974472ccdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 120)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in enumerate(list(embeddings[0])):\n",
        "  print(f\"{vectorizer.vocab[i]}-->{j}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6v0WYqX3QcW",
        "outputId": "5355778e-d3c4-4d08-d07a-1338e0052a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first-->[ 0.03838825 -0.05927904  0.07840646  0.04125115 -0.08230995  0.07507766\n",
            "  0.09762581  0.045667   -0.09867255  0.0624503   0.07419965 -0.09086895\n",
            "  0.05858266 -0.10045303  0.09924387  0.03329634  0.08240353 -0.07684124\n",
            " -0.05686071 -0.04822601  0.07820345  0.05972499  0.01935342  0.02556623\n",
            "  0.09234457 -0.01956867  0.04068892 -0.06160046 -0.07195235  0.03452663\n",
            " -0.09757455  0.09166995  0.04848677 -0.08157872 -0.04222412  0.08523004\n",
            " -0.04002318  0.08769921  0.07150321 -0.05587499 -0.09701591 -0.09103511\n",
            "  0.05098839  0.04793097  0.06309173  0.00383381  0.05290581 -0.08663704\n",
            " -0.0557266  -0.04753168  0.05469995  0.03938609 -0.07055417  0.03451018\n",
            "  0.08698063  0.06349916  0.05222734 -0.06417726  0.08591545 -0.03372167\n",
            " -0.04212867 -0.07595556 -0.06894468 -0.04489912 -0.06312299  0.05461016\n",
            "  0.07891735 -0.06174817 -0.04056623 -0.03440571 -0.0170647  -0.04761111\n",
            "  0.03354085  0.09936614  0.08427736  0.07575633  0.04813368  0.04192816\n",
            "  0.05055092 -0.04907617 -0.07640123 -0.06295482 -0.0681948   0.08776137\n",
            " -0.06388512 -0.08279042  0.05266503 -0.04543241  0.06314327  0.04884192\n",
            " -0.05241648  0.04910112 -0.06750758  0.07345134  0.08590982  0.07206131\n",
            "  0.07744514  0.06920582 -0.07456467 -0.0846033   0.04707657 -0.075852\n",
            "  0.06855246  0.05725192 -0.08208453  0.02843974 -0.04571832  0.06862565\n",
            " -0.0751134  -0.05559238 -0.03920883  0.09439072  0.07039765  0.04190402\n",
            " -0.01932508  0.06382774  0.04579802  0.08365129  0.03688437  0.06809138]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drnSHuPR32EG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}